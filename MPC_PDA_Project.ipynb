{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSDJAT7W6C1i"
   },
   "source": [
    "# Installing necessities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b_CLGvBPKjCl",
    "outputId": "8877b856-bfed-44bf-ddf9-dd123a30ee51"
   },
   "outputs": [],
   "source": [
    "# Installing necessary packages\n",
    "\n",
    "!pip install ale-py # Package called \"The Arcade Learning Environment (ALE)\" which allows to develop AI agents for Atari rooms.\n",
    "!pip install gymnasium # OpenAI's Gymnazium package.\n",
    "!pip install --upgrade keras # Keras package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMsU8ERJ4VEh"
   },
   "source": [
    "# Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bCIVcIwAQ1jd"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import ale_py\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# Source #1: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html#dqn-algorithm\n",
    "\n",
    "# Setting up our environment via ALE and Gymnazium, initializing Keras as we use Deep Q-Learning which is an approach for training reinforcment learning agents fit for games.\n",
    "# Mathplolib is also being set up here as we'll generate graph at the end as a demonstration of the learning process.\n",
    "\n",
    "gym.register_envs(ale_py) # Registering Atari environments.\n",
    "env = gym.make('ALE/Pacman-ram-v5', render_mode=\"rgb_array\") # Picking our own environment out of all that Atari offers.\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pYLDOmIgZLW3"
   },
   "outputs": [],
   "source": [
    "# At this point we set something called \"Replay Memory\". This approach stores each transition (transition=agent's interaction with the environment) which is later observed by the agent, allowing it to reuse the data.\n",
    "# Moreover, this approach improves learning procedure significantly.\n",
    "\n",
    "# Two classes are required:\n",
    "  ## Transition - represents a single transition in the environment. It maps pairs (state, action) to their corresponding result (next state, reward.)\n",
    "    ### State - current state of the game (current state of the board). In Pacman it's the current position of the player, ghost, food, etc.\n",
    "    ### Action - helps agent to \"think\" and make decision that affects current game state. In Pacman it could be moving left, right, up, down, etc.\n",
    "    ### next_state - State after action has taken place. It represents update to grid (game borad). This provides agent with update regarding consequences of his action/s.\n",
    "    ### Reward - A numerical value whcih given to the agent after each action. It serves as a reflection of how good/bad the action was or how good/bad what its impact on the game.\n",
    "  ## ReplayMemory - This is a buffer that holds transition obsorved recently.\n",
    "\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2JPdUP-_4in-"
   },
   "source": [
    "# Setting up Deep Q Learning network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nt8lgqVWJ6FF"
   },
   "outputs": [],
   "source": [
    "# Q-Learning and Q-values\n",
    "## In Deep Q Learning our goal is to approximate the Q-values for each action in every state. During training the neural network takes a state as an input and outputs Q-values for all possible actions (action evaluation).\n",
    "## The Q-value of a state-action pair represents the expected cumulativ reward agent receives by taking this action.\n",
    "## Based on those Q-values the agent performs \"optimal action-selection\".\n",
    "\n",
    "\n",
    "# Q-Network\n",
    "## In our implementation of Q-network we use something called \"feed-forward network\" whose primary goal is to approximate aformentioned Q-values for each possible action at certain state.\n",
    "## The network takes current game state (grid state) as an input and \"observes\" chages and differences between current state and the previous one.\n",
    "## Subsequently, network calculates and returns Q-values for each possible action where each value represent a reward that agent gets in case it decides to take this action.\n",
    "\n",
    "## Input [curent_state] -> Q-Network -> Q(current_state, action)\n",
    "\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions): # n_observations - number of observations (states) in the environment, n_actions - number of actions that agent can take.\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 256) \n",
    "        self.layer2 = nn.Linear(256, 256)\n",
    "        self.layer3 = nn.Linear(256, n_actions) # The output of the network is a vector of n_actions elements, each element represents Q-value for each action.\n",
    "\n",
    "\n",
    "\n",
    "    # The function below is called \"Activation function\". Activation fucntion is applied on network's output at each layer and server for introduction of non-linearity thus allow the network to learn complex patterns.\n",
    "    # Varous activation functions exists in the example below ReLU is used. ReLU returns output directly in case the input is positive otherwise returns zero.\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(start_dim=1) # Flattening the input tensor to 1D tensor.\n",
    "        x = F.relu(self.layer1(x)) # Applying ReLU activation function on the output of the first layer.\n",
    "        x = F.relu(self.layer2(x)) \n",
    "        return self.layer3(x) # Final layer produces n_actions ( Q(current_state, action) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Zk8fEX440gc"
   },
   "source": [
    "# Preparing for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lAyBLgKLcPXW",
    "outputId": "3ec2a4a2-cfe7-44d8-809c-d88c6fcd939a"
   },
   "outputs": [],
   "source": [
    " # Training\n",
    "\n",
    "## Here we instantiate our model and its optimizers plus some utulities.\n",
    "### select_action - sometimes our model gets to pick the following action sometimes one is picked uniformly, probability of that starts at EPS_START and will decay (controlle by EPS_DECAY) exponentialy towards EPS_END\n",
    "### plot_duration - for plotting duration episodes\n",
    "\n",
    "\n",
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "BATCH_SIZE = 256 \n",
    "GAMMA = 0.95\n",
    "EPS_START = 1.0 # Start with full exploration\n",
    "EPS_END = 0.05 # Minimal exploration toward the end\n",
    "EPS_DECAY = 50000 # Exploration vs exploitation\n",
    "TAU = 0.005\n",
    "LR_START = 1e-3 \n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "print(state.shape)\n",
    "n_observations = np.prod(state.shape)  # Calculate total elements after flattening\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR_START, amsgrad=True, weight_decay=0.006) # Optimizer for the network. \n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.7, patience=10, verbose=True) # Scheduler for learning rate\n",
    "memory = ReplayMemory(100000) # Store previous transitions\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY) # Exponential decay of epsilon\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1).indices.view(1, 1) # Pick the action with the highest Q-value\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long) # Randomly pick an action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CnnnOKF4-OI"
   },
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IlO0ZbOUhTsU"
   },
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE) # Sample a random batch of transitions\n",
    "    batch = Transition(*zip(*transitions)) \n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6_3_XmX89_H"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sge5X8SH8-f4"
   },
   "outputs": [],
   "source": [
    "# Evaluation Function\n",
    "def evaluate_agent(env, agent, device, episode, render=False):\n",
    "    state, _ = env.reset() \n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0) # Convert state to tensor\n",
    "    total_reward = 0\n",
    "    if render:\n",
    "      env.start_recording(video_name=f\"episode_{episode + 1}\")\n",
    "\n",
    "    for t in count():\n",
    "        with torch.no_grad():\n",
    "            action = agent(state).max(1).indices.view(1, 1)\n",
    "            if t < 30:\n",
    "                action = torch.tensor([[2]], device=device, dtype=torch.long) # Sometimes the agent gets stuck, so we force it to move up\n",
    "\n",
    "        observation, reward, terminated, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "        state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0) # Update state\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "# Logging Function\n",
    "def log_metrics(total_rewards):\n",
    "    print(f\"Mean Reward: {np.mean(total_rewards)}\")\n",
    "    print(f\"Max Reward: {np.max(total_rewards)}\")\n",
    "    print(f\"Min Reward: {np.min(total_rewards)}\")\n",
    "    print(f\"Standard Deviation: {np.std(total_rewards)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rBNbjfKY5GbT"
   },
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wJvMKS3o2hW-"
   },
   "outputs": [],
   "source": [
    "def show_frame(env):\n",
    "    # Render the current state of the environment as an image\n",
    "    frame = env.render()\n",
    "    plt.imshow(frame)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KE-8Nyhr0Qhp"
   },
   "outputs": [],
   "source": [
    "# Monitor the reward\n",
    "rewards = []\n",
    "episode_avg_losses = []\n",
    "episode_durations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "l4XpeBEFnlwV",
    "outputId": "1807cf61-3617-490f-beaf-1b90b2d429fe"
   },
   "outputs": [],
   "source": [
    "num_episodes = 12000\n",
    "evaluation_interval = 20  # Evaluate every 100 episodes\n",
    "evaluation_episodes = 5   # Number of episodes to run during evaluation\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    \n",
    "    # Save the model every 1000 episodes\n",
    "    if i_episode % 1000 == 0 and i_episode > 0:\n",
    "        torch.save(policy_net.state_dict(), f\"policy_net_{i_episode}.pth\")\n",
    "        torch.save(target_net.state_dict(), f\"target_net_{i_episode}.pth\")\n",
    "        print(f\"Checkpoint saved at episode {i_episode}\")\n",
    "\n",
    "    # Initialize the environment and get its state\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    total_reward = 0\n",
    "    episode_loss = []\n",
    "    \n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item()) # Perform the action and observe the next state and reward\n",
    "        reward = torch.tensor([reward], device=device) \n",
    "        total_reward += reward\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None \n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward) \n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        step_loss = optimize_model()\n",
    "\n",
    "        if step_loss is not None:\n",
    "          episode_loss.append(step_loss)\n",
    "\n",
    "        # save model\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        for key in policy_net.state_dict():\n",
    "          target_net.state_dict()[key].data.copy_(\n",
    "          TAU * policy_net.state_dict()[key].data + (1 - TAU) * target_net.state_dict()[key].data\n",
    "        )\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            rewards.append(total_reward)\n",
    "            if (len(episode_loss) > 0):\n",
    "              avg_loss = sum(episode_loss) / len(episode_loss)\n",
    "            else:\n",
    "              avg_loss = 0\n",
    "            episode_avg_losses.append(avg_loss)\n",
    "            break\n",
    "\n",
    "    # Evaluation phase\n",
    "    if i_episode % evaluation_interval == 0:\n",
    "        round_rewards = []\n",
    "        for episode in range(evaluation_episodes):\n",
    "          round_reward = evaluate_agent(env, policy_net, device, episode)\n",
    "          round_rewards.append(round_reward)\n",
    "        avg_reward = sum(round_rewards) / len(round_rewards)\n",
    "        print(f\"Trained episode {i_episode: >5}. Avg reward in gameplay: {avg_reward} points\")\n",
    "        if avg_reward > 500: # If the agent gets an average reward of 500 points, we consider the environment solved, preventing overfitting.\n",
    "          print(f\"Environment solved in {i_episode} episodes!\")\n",
    "          break\n",
    "        \n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hp0n90fe5J-R"
   },
   "source": [
    "# Plot training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-yL7kZpo0gt3",
    "outputId": "c71b81fc-40ff-4503-fcdc-d65ca634cdbd"
   },
   "outputs": [],
   "source": [
    "def plot_rewards(rewards):\n",
    "    rewards = [reward.cpu().item() for reward in rewards]\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(rewards, label=\"Reward per Epoch\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(\"Training Progress: Reward Per Epoch\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss(avg_losses):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(avg_losses, label=\"Average Loss per Episode\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Progress: Loss Per Episode\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Result')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "# Plot rewards\n",
    "plot_durations()\n",
    "plot_rewards(rewards)\n",
    "plot_loss(episode_avg_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQNJOxme5V_M"
   },
   "source": [
    "# Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J7cc2Hjy6nZY"
   },
   "outputs": [],
   "source": [
    "# Save the model's state dictionary\n",
    "torch.save(policy_net.state_dict(), \"policy_net_v10_2840e.pth\")\n",
    "torch.save(target_net.state_dict(), \"target_net_v10_2840e.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOjLhIh46zCm"
   },
   "source": [
    "# Download and upload to avoid constant training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZK2M0ETz65Bx"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Download the files\n",
    "files.download(\"policy_net_v4_200e.pth\")\n",
    "files.download(\"target_net.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "id": "buq_ama07GNs",
    "outputId": "36227f21-7e4f-49a9-d7f0-f003839f1bcb"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Upload the model file\n",
    "uploaded = files.upload() \n",
    "\n",
    "# The uploaded file will be saved in the Colab workspace\n",
    "# Get the filename\n",
    "filename = list(uploaded.keys())[0]\n",
    "print(f\"Uploaded file: {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vB8NrmC55cLg"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Ec3sAWI0Fsj8",
    "outputId": "460fcc58-7aad-4e4e-b8e0-89cf9a70cbe3"
   },
   "outputs": [],
   "source": [
    "import ale_py\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import os\n",
    "from itertools import count\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create and wrap the environment\n",
    "gym.register_envs(ale_py)\n",
    "env = gym.make('ALE/Pacman-ram-v5', render_mode=\"rgb_array\")\n",
    "env = gym.wrappers.RecordVideo(env, \"videos\", episode_trigger=lambda t: True)\n",
    "\n",
    "# Initialize agent\n",
    "agent = DQN(n_observations, n_actions).to(device)\n",
    "agent.load_state_dict(torch.load(\"policy_net_v10_2840.pth\"))\n",
    "agent.eval()\n",
    "\n",
    "# Plotting Function\n",
    "def plot_rewards(rewards):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(range(1, len(rewards) + 1), rewards, color='blue', label='Episode Reward')\n",
    "    plt.xlabel(\"Episode\", fontsize=14)\n",
    "    plt.ylabel(\"Reward\", fontsize=14)\n",
    "    plt.title(\"Agent Evaluation: Rewards Per Episode\", fontsize=16)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.axhline(np.mean(rewards), color='red', linestyle='--', label='Mean Reward')  # Add a mean line\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "# Evaluate Over Multiple Episodes\n",
    "n_episodes = 10\n",
    "rewards = []\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    reward = evaluate_agent(env, agent, device, episode, render=True) # Evaluate the agent over multiple episodes\n",
    "    env.close() \n",
    "    rewards.append(reward)\n",
    "    print(f\"Episode {episode + 1}/{n_episodes}: Reward = {reward}\")\n",
    "\n",
    "log_metrics(rewards)\n",
    "plot_rewards(rewards)\n",
    "\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAfDJ0eR5fmh"
   },
   "source": [
    "# Display video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Om6CHukOKMvL",
    "outputId": "c62bceb4-6674-46b3-dc00-54e4478d450a"
   },
   "outputs": [],
   "source": [
    "!ls videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "id": "ltBtAui-WxBf",
    "outputId": "7b80ed2b-05c2-44ac-908a-cc0e55e09025"
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "mp4 = open('videos/episode_7.mp4','rb').read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "HTML(\"\"\"\n",
    "<video width=400 controls>\n",
    "      <source src=\"%s\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\" % data_url)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
